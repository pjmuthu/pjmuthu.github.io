<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
    <style>
        body {
            display: flex;
            flex-direction: column;
            justify-content: flex-start; /* Align content at the top vertically */
            align-items: center;
            height: 100vh;
            margin: 0;
            background-color: #E8E8E8; /* Set your desired background color here */
        }

        header {
            height: 80px;
            width: 100%;
            background-color: #333; /* Set your desired header background color here */
            display: flex;
            justify-content: center;
            align-items: center;
            color: #FFF; /* Set your desired text color for the header */
        }

        canvas {
            border: 4px solid #000; /* Optional: Add a border for better visibility */
            margin-top: 10px; /* Adjust margin-top as needed */
        }
    </style>
  </head>
  <body>
    <header>
      <p> </p>
      <p> </p>
    </header>
    <p> <canvas id="canvas" oncontextmenu="event.preventDefault()"></canvas></p>
    <h1> <span style="font-family: Helvetica,Arial,sans-serif;">
        <script type="text/javascript">
        var Module = {
            canvas: (function () { return document.getElementById('canvas'); })()
        };
    </script>
        <script src="index.js" async=""></script>RPS AI<br>
      </span></h1>
    <blockquote><span style="font-family: Helvetica,Arial,sans-serif;">This web
        application is more of a sandbox than a game, designed to showcase the
        real-time capabilities of neural networks for generalized pattern
        recognition. The backend AI features a custom-built feedforward neural
        network, written in pure C with a compact 16 KB footprint. The algorithm
        aims to be accurate, fast, and lightweight, making it suitable for
        efficient deployment in various applications, including embedded
        systems.</span><br>
      <span style="font-family: Helvetica,Arial,sans-serif;"></span>
      <p><span style="font-family: Helvetica,Arial,sans-serif;"><br>
          While similar concepts have been explored in recent publications or
          websites, this project distinguishes itself by focusing on practical
          real-time applications rather than post-processing training sets.
          Notably, the neural network is compiled in WebAssembly, providing
          superior performance and speed compared to slower JavaScript
          implementations commonly seen in similar applications. This approach
          aims to demonstrate the efficiency and speed achievable even within a
          web browser environment.<br>
          <br>
        </span></p>
    </blockquote>
    <h1 style="text-align: center;"><span style="font-family: Helvetica,Arial,sans-serif;">How
        to Play<br>
      </span></h1>
    <blockquote>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">Rock, Paper,
          Scissors is a straightforward game played between two participants.
          The gameplay follows a cyclic principle, where each shape defeats one
          of the other two and is in turn defeated by the remaining one. The
          game has four possible outcomes for each round:<br>
        </span></p>
      <ul>
        <li><span style="font-family: Helvetica,Arial,sans-serif;">Rock crushes
            Scissors. <br>
          </span></li>
        <li><span style="font-family: Helvetica,Arial,sans-serif;">Scissors cut
            Paper. <br>
          </span></li>
        <li><span style="font-family: Helvetica,Arial,sans-serif;">Paper covers
            Rock. <br>
          </span></li>
        <li><span style="font-family: Helvetica,Arial,sans-serif;">If both
            players choose the same shape, the round results in a tie. <br>
          </span></li>
      </ul>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">In this
          implementation, players can click the corresponding icon or use
          numeric keys 1, 2, 3 for rock, paper, and scissors, respectively. <br>
        </span></p>
    </blockquote>
    <h2><span style="font-family: Helvetica,Arial,sans-serif;">AI Controls</span></h2>
    <blockquote>
      <p><span style="font-family: Helvetica,Arial,sans-serif;"> Holding the
          shift key reveals additional information, providing insights into the
          game dynamics, including win rates for both the player and the
          computer. The AI's estimated probability of the player's choice is
          displayed, along with confidence levels in using short-term or
          long-term memory. <br>
        </span></p>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">Players can
          toggle between different settings: <br>
        </span></p>
      <ul>
        <li><span style="font-family: Helvetica,Arial,sans-serif;">Computer
            Playstyle: Options include attempting to win, intentionally choosing
            the losing option, or mimicking the player's style. <br>
          </span></li>
        <li><span style="font-family: Helvetica,Arial,sans-serif;">Computer
            Memory Length: Allows the AI to access either short-term or
            long-term memory for decision-making. <br>
          </span></li>
      </ul>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">These settings
          showcase the AI's versatility in various applications and adaptability
          to different player skill levels. <br>
        </span></p>
    </blockquote>
    <h1><span style="font-family: Helvetica,Arial,sans-serif;">Machine Learning</span></h1>
    <blockquote><span style="font-family: Helvetica,Arial,sans-serif;">The
        general AI model comprises two models: a short-term model, reacting to
        immediate player changes (learning patterns and adapting within 2-16
        moves); a long-term model storing player biases with approximately 16
        examples of play.</span><br>
      <span style="font-family: Helvetica,Arial,sans-serif;"></span>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">The specific
          models used involves feedforward neural networks, where n-grams serve
          as inputs, and the outputs are always three (for the three options:
          Rock, Paper, Scissors). A single hidden layer was deemed sufficient,
          as additional complexity did not yield performance improvements.
          Various encoding methods were attempted, with the most effective being
          numerical double encoding (-1 for Rock, 0 for Paper, +1 for Scissors).
          While this encoding is not the most efficient, the intention was for
          subsequent neurons to capture the issue of zero efficiency. Explicitly
          encoding inputs was challenging due to exponential nature, and
          sequence-based neural networks like LSTM were not implemented at the
          time.<br>
          <br>
          Each model provides a score (from -3 to 3) based on the last five
          hands (win = 1, tie = 0, loss = -1), aiming to identify general
          changes in strategies. The predicted outcome for each turn, from each
          model, is weighted by the current score to determine the overall AI
          move. For instance, if the short-term model is outperforming others
          due to recent successful choices by the player, its decisions will
          override results from the other models.<br>
          <br>
          To evaluate the models, data was collected across 15,000 games
          involving 50 players. In principle, this model should be able to
          defeat any player over time due to external data for training. The
          underlying assumption is that human responses are not purely random,
          and the control case, resembling a simple rule (if the player does x,
          then I do y), achieved a 41% win rate, surpassing random chance (33%).</span></p>
      <p></p>
      <h2 style="text-align: center;"><span style="font-family: Helvetica,Arial,sans-serif;">Short-Term
          Model</span></h2>
      <h2><span style="font-family: Helvetica,Arial,sans-serif;"></span></h2>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">The short-term
          model is a (3, 1, 2, 3) model with 2 inputs, 1 hidden layer, 4
          neurons, and 3 outputs. It is expected to achieve a performance of
          44%, slightly better than a simple 1-gram control and random. This
          model rapidly adapts to input changes and trains on the last 16
          observations (17 total weights), potentially learning new short
          patterns from players attempting to confuse the AI with variant
          patterns.<br>
        </span></p>
      <h2 style="text-align: center;"><span style="font-family: Helvetica,Arial,sans-serif;">Long-Term
          Model</span></h2>
      <h2><span style="font-family: Helvetica,Arial,sans-serif;"></span></h2>
      <p><span style="font-family: Helvetica,Arial,sans-serif;">The long-term
          model is a (4, 1, 5, 3) model with a higher performance of 57%. It
          aims to learn long-term patterns from players using more complex
          logic. This model trains on the last 32 observations (38 total
          weights), potentially capturing specific player biases, and remains
          persistent between plays. If a player adopts a wholly different
          strategy, the AI relies on the short-term model until enough data is
          available to learn the new strategy. If a player stops a strategy and
          resumes their existing playstyle, the long-term model recovers.<br>
          <br>
        </span></p>
    </blockquote>
  </body>
</html>
